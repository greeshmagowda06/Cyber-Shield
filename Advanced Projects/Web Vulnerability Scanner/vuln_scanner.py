"""
Passive web vulnerability scanner (educational & defensive).
Performs non-invasive checks like HTTPS, security headers, robots.txt.
DO NOT use this script to scan systems without permission.
"""

import requests
from urllib.parse import urljoin, urlparse

SECURE_HEADERS = [
    "Content-Security-Policy",
    "X-Frame-Options",
    "X-Content-Type-Options",
    "Referrer-Policy",
    "Strict-Transport-Security"
]

def fetch(url, timeout=8):
    try:
        r = requests.get(url, timeout=timeout, allow_redirects=True)
        return r
    except Exception as e:
        print(f"[ERROR] Request failed: {e}")
        return None

def check_https(url):
    parsed = urlparse(url)
    if parsed.scheme == "https":
        return True
    # try HTTPS
    https_url = parsed._replace(scheme="https").geturl()
    r = fetch(https_url)
    return bool(r and r.status_code < 400)

def check_headers(resp):
    missing = []
    for h in SECURE_HEADERS:
        if h not in resp.headers:
            missing.append(h)
    return missing

def check_robots(url):
    parsed = urlparse(url)
    robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
    r = fetch(robots_url)
    return (r is not None and r.status_code == 200, r.text if r is not None else "")

def run_checks(url):
    print(f"Scanning (passive): {url}\n")
    # ensure scheme
    if not url.startswith("http"):
        url = "http://" + url

    r = fetch(url)
    if not r:
        print("Failed to fetch the URL.")
        return

    print(f"Final URL: {r.url}")
    print(f"Status Code: {r.status_code}")
    print(f"Server Header: {r.headers.get('Server', 'N/A')}")

    https_ok = check_https(url)
    print(f"HTTPS available: {https_ok}")

    missing = check_headers(r)
    if missing:
        print("Missing security headers:")
        for h in missing:
            print(" -", h)
    else:
        print("All common security headers present (CSP/X-Frame/etc) - good.")

    has_robots, robots_text = check_robots(url)
    print(f"robots.txt present: {has_robots}")
    if has_robots:
        print("--- robots.txt (first 300 chars) ---")
        print(robots_text[:300])

    # simple check: does site reflect a simple benign test string in body?
    test_marker = "<!--scanner-test-->"
    if test_marker in r.text:
        print("Note: test marker found in body (developer left marker).")

if __name__ == "__main__":
    target = input("Enter target site (e.g., example.com or https://example.com): ").strip()
    run_checks(target)
